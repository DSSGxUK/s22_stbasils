{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cc592771",
   "metadata": {},
   "source": [
    "## Testing the new cluster generated by Hannah (10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "be6acfcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: imblearn in /opt/conda/lib/python3.9/site-packages (0.0)\n",
      "Requirement already satisfied: imbalanced-learn in /opt/conda/lib/python3.9/site-packages (from imblearn) (0.9.1)\n",
      "Requirement already satisfied: joblib>=1.0.0 in /opt/conda/lib/python3.9/site-packages (from imbalanced-learn->imblearn) (1.0.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.9/site-packages (from imbalanced-learn->imblearn) (3.0.0)\n",
      "Requirement already satisfied: scipy>=1.3.2 in /opt/conda/lib/python3.9/site-packages (from imbalanced-learn->imblearn) (1.7.1)\n",
      "Requirement already satisfied: numpy>=1.17.3 in /opt/conda/lib/python3.9/site-packages (from imbalanced-learn->imblearn) (1.20.3)\n",
      "Requirement already satisfied: scikit-learn>=1.1.0 in /opt/conda/lib/python3.9/site-packages (from imbalanced-learn->imblearn) (1.1.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install imblearn\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c26c586",
   "metadata": {},
   "source": [
    "### Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2a7b1b15",
   "metadata": {},
   "outputs": [],
   "source": [
    "#All import go here\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "#Get and describe data\n",
    "path = 'temp.csv'\n",
    "df = pd.read_csv(path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8e1c5a9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1    1739\n",
      "0     485\n",
      "Name: Result, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import pickle\n",
    "\n",
    "########################################\n",
    "##Temperoroy dropping of some variables\n",
    "######################################\n",
    "\n",
    "df1 = df.drop(['Sexual Orientation','D1','J1','Medical Issue','Nationality','Gender','Marital Status','Intrptr Reqd','L1','Immigration Status'],1)\n",
    "\n",
    "le = LabelEncoder()\n",
    "df1['Result'] = le.fit_transform(df1['Result'].values)\n",
    "print(df1['Result'].value_counts())\n",
    "#Separate training features from target\n",
    "X = df1.drop(['Result'], axis=1)\n",
    "y = df1['Result']\n",
    "\n",
    "####### \n",
    "#Train dataset = X\n",
    "#Test Dataset = y\n",
    "########\n",
    "  \n",
    "# Convert the following numerical labels from interger to float\n",
    "ss = StandardScaler()\n",
    "X[['Session per week','TotalDisabilty','TotalMentalHealth','Time per Session']] = ss.fit_transform(X[['Session per week','TotalDisabilty','TotalMentalHealth','Time per Session']])\n",
    "output = open('StandardScaler.pkl', 'wb')\n",
    "pickle.dump(ss, output)\n",
    "output.close()\n",
    "\n",
    "# categorical features to be converted to One Hot Encoding\n",
    "#'Intrptr Reqd','Full Assessment','EET status'\n",
    "categ = [ 'Acc Type prev', 'B1', 'C1','Economic Status', 'Area', 'Scheme','EET status','Service Type','Religion',]\n",
    "# One hot encoding the features with more than two categories\n",
    "ohe = OneHotEncoder()\n",
    "X_object = X[categ]\n",
    "X = X.drop(categ,1)\n",
    "ohe.fit(X_object)\n",
    "codes = ohe.transform(X_object).toarray()\n",
    "feature_names = ohe.get_feature_names(categ)\n",
    "X = pd.concat([X,  pd.DataFrame(codes,columns=feature_names).astype(int)], axis=1)\n",
    "\n",
    "output = open('OneHotEncoder.pkl', 'wb')\n",
    "pickle.dump(ohe, output)\n",
    "output.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a81c14af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results diplayed here !!\n"
     ]
    }
   ],
   "source": [
    "for c in df1.columns:\n",
    "    if(c in ['TotalDisabilty','TotalMentalHealth','Time per Session','Session per week']):\n",
    "        continue\n",
    "    print(\"{} column unique values {}\".format(c, df1[c].unique()))\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc3ddcf8",
   "metadata": {},
   "source": [
    "### Train Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a3cbd893",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now split the data into train and test Split \n",
    "seed = 999\n",
    "X_Train, X_val, y_Train, y_val = train_test_split(X, y, stratify = y, test_size = 0.2, random_state=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cf0f327e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train and Tets data are :\n",
      "(1779, 72)\n",
      "(445, 72)\n",
      "Teh ditribution of teh train data is :\n",
      "1    1391\n",
      "0     388\n",
      "Name: Result, dtype: int64\n",
      "Teh ditribution of teh test data is :\n",
      "1    348\n",
      "0     97\n",
      "Name: Result, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(\"Train and Tets data are :\")\n",
    "print(X_Train.shape)\n",
    "print(X_val.shape)\n",
    "print(\"Teh ditribution of teh train data is :\")\n",
    "print(y_Train.value_counts())\n",
    "print(\"Teh ditribution of teh test data is :\")\n",
    "print(y_val.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b2c377af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    1391\n",
       "0    1391\n",
       "Name: Result, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Borderline SMOTE with SVM\n",
    "from imblearn.over_sampling import SVMSMOTE\n",
    "\n",
    "oversample = SVMSMOTE()\n",
    "X1, y1 = oversample.fit_resample(X_Train, y_Train)\n",
    "y1.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc1ad628",
   "metadata": {},
   "source": [
    "### Fit the model on Training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2c8dab9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After Threshold Tuning, results are:\n",
      "\n",
      "F1:0.593 Threshold=0.44\n",
      "\n",
      "\n",
      "\n",
      "The confusion matrix for maxium score is :\n",
      "[[ 31  66]\n",
      " [ 50 298]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.38      0.32      0.35        97\n",
      "           1       0.82      0.86      0.84       348\n",
      "\n",
      "    accuracy                           0.74       445\n",
      "   macro avg       0.60      0.59      0.59       445\n",
      "weighted avg       0.72      0.74      0.73       445\n",
      "\n"
     ]
    }
   ],
   "source": [
    "###################################################\n",
    "### Finally training the model with best params ###\n",
    "###################################################\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def to_labels(pos_probs, threshold):\n",
    "    return (pos_probs >= threshold).astype('int')\n",
    "\n",
    "best_model = RandomForestClassifier(criterion='log_loss',max_depth = 6, n_estimators=80)\n",
    "best_model = best_model.fit(X1,y1)\n",
    "\n",
    "# Do the Threshold tuning and check for the possible result we can get at any threshold\n",
    "yhat = best_model.predict_proba(X_val)\n",
    "probs = yhat[:,1]\n",
    "# define thresholds\n",
    "thresholds = np.arange(0, 1, 0.001)\n",
    "# evaluate each threshold\n",
    "scores = [f1_score(y_val, to_labels(probs, t), average='macro') for t in thresholds]\n",
    "\n",
    "# get best threshold\n",
    "ix = np.argmax(scores)\n",
    "\n",
    "max_score_on_val = scores[ix]\n",
    "#final_model_RF = model\n",
    "y_pred = np.zeros((len(X_val),1), np.uint8)\n",
    "for j in range(len(X_val)):\n",
    "    y_pred[j] = to_labels(probs[j], thresholds[ix])\n",
    "confusion = confusion_matrix(y_val, y_pred)\n",
    "print(\"After Threshold Tuning, results are:\\n\")\n",
    "print('F1:%.3f Threshold=%.4f' % (scores[ix],thresholds[ix]))\n",
    "print(\"\\n\\n\")\n",
    "print(\"The confusion matrix for maxium score is :\")\n",
    "print(confusion)\n",
    "print(classification_report(y_val, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "11532931",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Saving the model to the pickle file \n",
    "import pickle\n",
    "\n",
    "# Optimal Thresh is 0.44\n",
    "output = open('model_final.pkl', 'wb')\n",
    "pickle.dump(best_model, output)\n",
    "output.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "17b072bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tree here !\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.tree import plot_tree\n",
    "\n",
    "fig = plt.figure(figsize=(15, 10))\n",
    "data = plot_tree(best_model.estimators_[5], \n",
    "          feature_names=X1.columns,\n",
    "          class_names=['0','1'], \n",
    "          filled=True, impurity=True, \n",
    "          rounded=True)\n",
    "#fig.savefig('2224_mod_ADAsyn_RF.png', dpi=800)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a284171d",
   "metadata": {},
   "source": [
    "## Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "a08e8f2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "importances_unclustered = best_model.feature_importances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fa3e0eaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results here \n"
     ]
    }
   ],
   "source": [
    "print(\"Total features :\")\n",
    "print(len(best_model.feature_names_in_))\n",
    "print('\\n')\n",
    "best_model.feature_names_in_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "5ef8914f",
   "metadata": {},
   "outputs": [],
   "source": [
    "############for the Data 2224\n",
    "\n",
    "columns = ['TotalDisabilty', 'TotalMentalHealth', 'Time per Session',\n",
    "       'Session per week','Acc Type prev','B1', 'C1','Economic Status',\n",
    "       'Area', 'Scheme', 'EET status', 'Service Type', 'Religion']\n",
    "\n",
    "############for the Data 2224\n",
    "\n",
    "one_categ_columns = ['Medical Issue','D1','J1','L1','TotalDisabilty',\n",
    "       'TotalMentalHealth', 'Time per Session','Session per week']\n",
    "\n",
    "pointer = 0\n",
    "importances = []\n",
    "for cat in columns:\n",
    "    if(cat in one_categ_columns):\n",
    "        values = 1\n",
    "    else:\n",
    "        values = len(df1[cat].unique())\n",
    "    importance = sum(importances_unclustered[pointer:(pointer+values)])\n",
    "    importances.append(importance)\n",
    "    pointer=pointer+values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "08a0562d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total features detected are 72 with feature importance sum 0.9999999999999999\n"
     ]
    }
   ],
   "source": [
    "print(\"Total features detected are {} with feature importance sum {}\".format(pointer, sum(importances)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e8b6d029",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature Importances\n"
     ]
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "forest_importances = pd.Series(importances, index=columns)\n",
    "forest_importances = forest_importances.sort_values()\n",
    "fig, ax = plt.subplots()\n",
    "forest_importances.plot.barh(ax=ax)\n",
    "ax.set_title(\"Feature importances using MDI\")\n",
    "ax.set_ylabel(\"Mean decrease in impurity\")\n",
    "fig.tight_layout()\n",
    "\n",
    "plot_width, plot_height = (20,14)\n",
    "plt.rcParams['figure.figsize'] = (plot_width,plot_height)\n",
    "plt.rcParams['font.size']=22"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eebc3d5f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
